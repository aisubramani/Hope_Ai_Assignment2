{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6f1dc-e19c-401f-bc40-39f86e128dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding GenAi RAG set question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24b3a0e-4e24-4043-9438-1ab9cc4b07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY=\"key\"\n",
    "OPENAI_API_KEY = \"key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7dd8e-2057-4866-a526-371ae28071fb",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "Task: Implement a FAISS-based Retrieval System that fetches the most relevant document for query-based retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d9d9ca-1e4e-44b9-b081-a780e5fa968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document:\n",
      "RAG combines retrieval and generation.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Documents\n",
    "documents = [\n",
    "    \"RAG combines retrieval and generation.\",\n",
    "    \"FAISS is a vector search library.\",\n",
    "    \"Generative AI creates new content.\"\n",
    "]\n",
    "\n",
    "# 2. Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Create embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "# 4. Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "# 5. Query\n",
    "query = \"What is RAG?\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# 6. Search\n",
    "k = 1\n",
    "distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "# 7. Result\n",
    "print(\"Most relevant document:\")\n",
    "print(documents[indices[0][0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5373a6-2c34-40f6-947e-685565e82c3f",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "Task: Use Gemini API to generate answers based on retrieved knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee78c134-165b-4bad-bec2-3724d9876e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG combines retrieval and generation.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# 1. Create Gemini client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# 2. Retrieved document (from FAISS)\n",
    "retrieved_doc = \"RAG combines retrieval and generation.\"\n",
    "\n",
    "# 3. User query\n",
    "query = \"very short explain about RAG.\"\n",
    "\n",
    "# 4. Prompt with retrieved knowledge\n",
    "prompt = f\"\"\"\n",
    "Use the following information to answer the question.\n",
    "\n",
    "Information:\n",
    "{retrieved_doc}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generate answer\n",
    "response = client.models.generate_content(\n",
    "    model=\"models/gemini-2.5-flash\",\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "# 6. Output\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b938b-7efd-4e5f-80f8-8efda0f78ba8",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "Task: Implement FAISS with a hybrid (dense & sparse) vector search for improving information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ded881-d035-49d8-a61e-32caec60b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant document:\n",
      "RAG combines retrieval and generation.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Documents\n",
    "documents = [\n",
    "    \"RAG combines retrieval and generation.\",\n",
    "    \"FAISS is a vector search library.\",\n",
    "    \"Generative AI creates new content.\"\n",
    "]\n",
    "\n",
    "# 2. Dense embeddings (semantic)\n",
    "dense_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "dense_embeddings = dense_model.encode(documents)\n",
    "\n",
    "# 3. FAISS index (dense)\n",
    "dim = dense_embeddings.shape[1]\n",
    "dense_index = faiss.IndexFlatL2(dim)\n",
    "dense_index.add(np.array(dense_embeddings))\n",
    "\n",
    "# 4. Sparse embeddings (keywords)\n",
    "tfidf = TfidfVectorizer()\n",
    "sparse_embeddings = tfidf.fit_transform(documents).toarray()\n",
    "\n",
    "# 5. Query\n",
    "query = \"What is RAG?\"\n",
    "query_dense = dense_model.encode([query])\n",
    "query_sparse = tfidf.transform([query]).toarray()\n",
    "\n",
    "# 6. Dense search\n",
    "k = 3\n",
    "dense_scores, dense_ids = dense_index.search(np.array(query_dense), k)\n",
    "\n",
    "# 7. Sparse similarity (dot product)\n",
    "sparse_scores = np.dot(sparse_embeddings, query_sparse.T).flatten()\n",
    "\n",
    "# 8. Hybrid score (simple sum)\n",
    "final_scores = {}\n",
    "for i in range(len(documents)):\n",
    "    dense_score = -dense_scores[0][list(dense_ids[0]).index(i)] if i in dense_ids[0] else 0\n",
    "    sparse_score = sparse_scores[i]\n",
    "    final_scores[i] = dense_score + sparse_score\n",
    "\n",
    "# 9. Best document\n",
    "best_doc = max(final_scores, key=final_scores.get)\n",
    "\n",
    "print(\"Most relevant document:\")\n",
    "print(documents[best_doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259a5e5-cbef-4ad4-990f-79c8f1784c75",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "Task: Implement a Multi-Turn Conversational Chatbot using Gemini API that maintains context across multiple user interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4cf77d8-409e-453c-b316-500d1bc21209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: As an AI, I don't have feelings or a physical body, so I can't be \"fine\" in the human sense. However, I am functioning perfectly and ready to help you!\n",
      "\n",
      "How are you doing today? And what can I assist you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I do not have a name. I am a large language model, trained by Google.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# 1. Create Gemini client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"models/gemini-2.5-flash\"\n",
    "\n",
    "history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user message\n",
    "    history.append(\n",
    "        genai.types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[genai.types.Part(text=user_input)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=history\n",
    "    )\n",
    "\n",
    "    reply = response.text\n",
    "    print(\"Bot:\", reply)\n",
    "\n",
    "    # Add bot reply\n",
    "    history.append(\n",
    "        genai.types.Content(\n",
    "            role=\"model\",\n",
    "            parts=[genai.types.Part(text=reply)]\n",
    "        )\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45ff38-0911-47a9-a267-4656dc0b4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5:\n",
    "\n",
    "Task: Implement a FAISS-based retrieval-augmented generation (RAG) system using the Gemini API, where retrieved documents are used as context to generate more accurate and relevant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db9ed15-d50d-43d0-8d8d-58710a3c7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "RAG combines retrieval and generation to improve accuracy.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google import genai\n",
    "\n",
    "# 1. Gemini client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "MODEL_NAME = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# 2. Documents (knowledge base)\n",
    "documents = [\n",
    "    \"RAG combines retrieval and generation to improve accuracy.\",\n",
    "    \"FAISS is a fast vector similarity search library.\",\n",
    "    \"Gemini is a large language model by Google.\"\n",
    "]\n",
    "\n",
    "# 3. Load embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Create document embeddings\n",
    "doc_embeddings = embed_model.encode(documents)\n",
    "\n",
    "# 5. Create FAISS index\n",
    "dim = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "# 6. User query\n",
    "query = \"Explain RAG in short form\"\n",
    "\n",
    "# 7. Embed query\n",
    "query_embedding = embed_model.encode([query])\n",
    "\n",
    "# 8. Retrieve top document\n",
    "k = 1\n",
    "_, indices = index.search(np.array(query_embedding), k)\n",
    "retrieved_doc = documents[indices[0][0]]\n",
    "\n",
    "# 9. Create RAG prompt\n",
    "prompt = f\"\"\"\n",
    "Use the following information to answer the question.\n",
    "\n",
    "Context:\n",
    "{retrieved_doc}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# 10. Generate answer using Gemini\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_NAME,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "# 11. Output\n",
    "print(\"Answer:\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824ce15-7c0a-4a9e-8efa-cf518e9a518e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
