{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b01dc-8dd8-4ee9-845c-f3d733ad338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Gen Ai and RAG set question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def00a10-bade-48bd-b730-98f7aefaa321",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY=\"key\"\n",
    "OPENAI_API_KEY = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35442061-9371-4286-b522-c5ff52860d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"models/gemini-2.5-flash\",\n",
    "    contents=\"Hello!\"\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df6c015-8a21-487b-90bb-eb75f66ea4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 17768,  1029,   102,     0],\n",
       "        [  101,  4863, 11416,  6024,  9932,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = [\"What is RAG?\", \"Explain Generative AI.\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c3899-e4b9-41c9-9cac-c1d9d221d4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b5256546fd4d859931368c3518fcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15016\\anaconda3\\envs\\aiagent\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15016\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b11cd8bee7c4ad5a79fb15a8e9f5b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6353d8c666a4e559d5723f86b41dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4334e56acc5b43b9bd02d9d5768bef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0b52cf629a4a04abbbc3a4f8cc82e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d273494375aa40fb984ecafc35eec637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 30, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level programming language widely used in data science, AI, web development and automation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"t5-small\"\n",
    ")\n",
    "\n",
    "text = \"Python is a high-level programming language widely used in data science, AI, web development, and automation.\"\n",
    "\n",
    "summary = summarizer(\n",
    "    text,\n",
    "    max_length=30,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(summary[0][\"summary_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d304dbd6-a644-475a-9cd8-60eac602c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! RAG is a crucial technique in making large language models (LLMs) more useful, accurate, and up-to-date.\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "## What is RAG?\n",
      "\n",
      "**RAG stands for Retrieval-Augmented Generation.**\n",
      "\n",
      "It's a method that enhances the capabilities of Large Language Models (LLMs) by giving them access to up-to-date, external, or proprietary information, beyond what they were trained on. Essentially, it acts as a \"research assistant\" for the LLM.\n",
      "\n",
      "### The Core Problem RAG Solves:\n",
      "\n",
      "LLMs (like GPT-4, Claude, Llama, etc.) are trained on vast amounts of data, but this data has a few limitations:\n",
      "\n",
      "1.  **Data Cutoff:** Their training data is only current up to a certain point (e.g., September 2021 for some models). They don't know about recent events or new information.\n",
      "2.  **Proprietary/Domain-Specific Knowledge:** They don't have access to your company's internal documents, your personal notes, or highly specific domain knowledge that wasn't part of their general training.\n",
      "3.  **Hallucinations:** If an LLM doesn't know the answer to a question, it might confidently \"hallucinate\" or make up information that sounds plausible but is factually incorrect.\n",
      "4.  **Lack of Attribution:** Even when an LLM gives a correct answer, it can't typically tell you *where* it got that information from.\n",
      "\n",
      "**RAG addresses these issues by providing the LLM with relevant, external information *at the time of the query*.**\n",
      "\n",
      "## How RAG Works (Step-by-Step):\n",
      "\n",
      "RAG generally involves two main phases: an **Indexing Phase** (done offline, in advance) and a **Retrieval & Generation Phase** (done in real-time when a user asks a question).\n",
      "\n",
      "### Phase 1: Indexing/Preparation (Offline)\n",
      "\n",
      "1.  **Data Sources:** You identify the external knowledge base you want the LLM to use. This could be:\n",
      "    *   Documents (PDFs, Word files, text files)\n",
      "    *   Web pages\n",
      "    *   Databases\n",
      "    *   Internal wikis or knowledge bases\n",
      "    *   APIs\n",
      "2.  **Chunking:** The raw data is too large for an LLM to process all at once. So, it's broken down into smaller, manageable pieces called \"chunks\" or \"segments.\" (e.g., a few paragraphs from a document, or a single FAQ entry).\n",
      "3.  **Embedding:** Each of these text chunks is then converted into a numerical representation called a \"vector embedding.\" These embeddings capture the semantic meaning of the text. Chunks with similar meanings will have embeddings that are \"close\" to each other in a multi-dimensional space.\n",
      "4.  **Vector Database:** These vector embeddings (along with a reference back to their original text chunks) are stored in a specialized database called a \"vector database\" or \"vector store.\" This database is optimized for very fast similarity searches.\n",
      "\n",
      "### Phase 2: Retrieval & Generation (Online - at Query Time)\n",
      "\n",
      "1.  **User Query:** A user asks a question (e.g., \"What are the Q3 sales figures for product X?\").\n",
      "2.  **Query Embedding:** The user's question is also converted into a vector embedding using the *same embedding model* used during indexing.\n",
      "3.  **Similarity Search (Retrieval):** The system then performs a \"similarity search\" in the vector database. It finds the vector embeddings (and thus the original text chunks) that are most semantically similar to the user's query. These are the most relevant pieces of information from your external knowledge base.\n",
      "4.  **Context Augmentation:** The retrieved relevant text chunks are then prepended or inserted into the user's original query. This augmented prompt is sent to the LLM.\n",
      "    *   **Example Prompt:** \"Based on the following context, please answer the question. Context: [Retrieved sales report data]. Question: What are the Q3 sales figures for product X?\"\n",
      "5.  **LLM Generation:** The LLM receives this augmented prompt. Now, instead of relying solely on its pre-trained knowledge, it has specific, up-to-date, and relevant information directly provided within the prompt. It uses this context to generate a more accurate, factual, and grounded response.\n",
      "\n",
      "## Why is RAG Important? (Benefits)\n",
      "\n",
      "*   **Improved Accuracy & Reduced Hallucinations:** Provides factual basis, significantly reducing the likelihood of the LLM making things up.\n",
      "*   **Access to Up-to-Date Information:** Overcomes the LLM's training data cutoff, allowing it to answer questions about recent events or newly published information.\n",
      "*   **Domain-Specific Knowledge:** Enables LLMs to answer questions about private, proprietary, or highly specialized data (e.g., your company's product manuals, internal HR policies).\n",
      "*   **Cost-Effective:** Often more cost-effective than continually fine-tuning or retraining large LLMs on new data.\n",
      "*   **Faster Development:** Easier and quicker to implement than full model fine-tuning for new knowledge.\n",
      "*   **Attribution & Explainability:** Because the LLM is given specific context, it can often cite its sources (e.g., \"According to document A, sales were X...\").\n",
      "*   **Reduced Training Data Reliance:** Less need for constant model re-training.\n",
      "\n",
      "## Real-World Examples/Use Cases:\n",
      "\n",
      "*   **Customer Support Chatbots:** Answering questions about product features, troubleshooting, or order status using your company's latest documentation.\n",
      "*   **Internal Knowledge Bases:** Employees can ask questions about company policies, IT guidelines, or project documentation.\n",
      "*   **Legal Research:** A lawyer asking an LLM to summarize case law relevant to a specific legal question, drawing from a vast legal database.\n",
      "*   **Healthcare Information:** Doctors or patients querying medical databases for information about conditions, treatments, or drug interactions.\n",
      "*   **Personalized Content:** Recommending articles or products based on a user's specific history and a large catalog of available items.\n",
      "\n",
      "In essence, RAG makes LLMs much more practical and reliable by bridging the gap between their vast general knowledge and your specific, real-time, or proprietary information.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"models/gemini-2.5-flash\",\n",
    "    contents=\"Hello, explain RAG\"\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de38bb33-2218-4e30-ab15-ae0aaabef309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2054, 2003, 11416, 6024, 9932, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"What is Generative AI?\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135a166-198d-41c9-b88c-4179a9ebb5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
