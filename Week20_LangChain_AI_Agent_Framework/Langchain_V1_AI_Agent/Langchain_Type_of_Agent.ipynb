{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3576d3-56e4-439f-a0b1-7b19a4155be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1ï¸âƒ£ ReAct Agent (Reason + Act) â­ Most Important\n",
    "\n",
    "The LLM:\n",
    "\n",
    "Thinks step-by-step\n",
    "Chooses tools\n",
    "Uses observations\n",
    "Continues until done\n",
    "\n",
    "Best for\n",
    "\n",
    "RAG systems\n",
    "Search + tools\n",
    "AI assistants\n",
    "Healthcare / Finance bots\n",
    "\n",
    " #---------------------\n",
    "ðŸ§  ReAct Agent = Think â†’ Act â†’ Observe â†’ Answer\n",
    "What this example does\n",
    "\n",
    "User asks:\n",
    "\n",
    "â€œWhat is 10 plus 25?â€\n",
    "\n",
    "LLM will:\n",
    "\n",
    "Think: â€œI need to calculateâ€\n",
    "Act: Call add_numbers\n",
    "Observe: Get 35\n",
    "Answer: â€œ35â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116dfdd7-3d84-4a80-ad56-138ed519beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: The final answer is 35.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "REACT AGENT (LangChain v1 â€“ Compatible Version)\n",
    "\n",
    "This agent:\n",
    "- Thinks\n",
    "- Chooses a tool\n",
    "- Uses observation\n",
    "- Produces an answer\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM (Brain)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ----------- TOOL -----------\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# ----------- STEP 1: LLM THINKS -----------\n",
    "# LLM sees the question and decides to call a tool\n",
    "response = llm.bind_tools([add_numbers]).invoke(\n",
    "    \"What is 10 plus 25?\"\n",
    ")\n",
    "\n",
    "# ----------- STEP 2: TOOL IS CALLED -----------\n",
    "tool_call = response.tool_calls[0]          # LLM asked to call add_numbers\n",
    "tool_result = add_numbers.invoke(tool_call[\"args\"])\n",
    "\n",
    "# ----------- STEP 3: LLM USES OBSERVATION -----------\n",
    "final = llm.invoke(\n",
    "    f\"The result of the calculation is {tool_result}. \"\n",
    "    \"Give the final answer to the user.\"\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\", final.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab99a6e-2e29-463b-b85b-8989ab9fbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "2ï¸âƒ£ Tool-Calling Agent (OpenAI Functions Agent) â­ Enterprise Standard\n",
    "\n",
    "Uses structured JSON tool calls.\n",
    "\n",
    "Why companies use it\n",
    "\n",
    "Safer\n",
    "Deterministic\n",
    "No prompt-hacking\n",
    "Works with APIs\n",
    "\n",
    "Best for\n",
    "\n",
    "Backend automation\n",
    "Production systems\n",
    "Healthcare & banking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e2fb9e-cca7-4b5c-b2c6-367e7caf45ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: The final answer is 7, since the square root of 49 is 7.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM (Brain) â€“ supports structured tool calling\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- TOOL ----------------\n",
    "@tool\n",
    "def get_square(n: int) -> int:\n",
    "    \"\"\"Return the square of a number\"\"\"\n",
    "    return n * n\n",
    "\n",
    "# ---------------- STEP 1: LLM DECIDES ----------------\n",
    "# LLM receives the question and decides whether to call a tool\n",
    "response = llm.bind_tools([get_square]).invoke(\n",
    "    \"What is the square of 7?\"\n",
    ")\n",
    "\n",
    "# ---------------- STEP 2: TOOL IS CALLED ----------------\n",
    "tool_call = response.tool_calls[0]         # Structured JSON tool call\n",
    "tool_result = get_square.invoke(tool_call[\"args\"])\n",
    "\n",
    "# ---------------- STEP 3: LLM USES TOOL RESULT ----------------\n",
    "final = llm.invoke(\n",
    "    f\"The square is {tool_result}. \"\n",
    "    \"Give the final answer to the user.\"\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\", final.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b82e29-a4c4-46a3-8835-12a4b71c50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "3ï¸âƒ£ Conversational Agent\n",
    "\n",
    "An agent with memory.\n",
    "\n",
    "Best for\n",
    "\n",
    "Chatbots\n",
    "Support bots\n",
    "Personal assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f938f927-e579-4dc7-a3b2-d36e155dabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "The sum of 5 plus 6 is 11. If you have any more math questions or anything else you'd like to know, feel free to ask!\n",
      "The result of adding 10 to 11 is 21. If you have any more calculations or questions, just let me know!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM (Brain)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- TOOL ----------------\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# ---------------- SIMPLE MEMORY ----------------\n",
    "chat_history = []\n",
    "\n",
    "def chat(user_input):\n",
    "    # Add user message to memory\n",
    "    chat_history.append(f\"User: {user_input}\")\n",
    "\n",
    "    # Build full conversation\n",
    "    full_prompt = \"\\n\".join(chat_history)\n",
    "\n",
    "    # Let LLM decide whether to call tool\n",
    "    response = llm.bind_tools([add_numbers]).invoke(full_prompt)\n",
    "\n",
    "    # If LLM wants to call a tool\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_result = add_numbers.invoke(tool_call[\"args\"])\n",
    "\n",
    "        # LLM uses tool result\n",
    "        final = llm.invoke(\n",
    "            f\"{full_prompt}\\nTool result: {tool_result}\\nGive a helpful reply.\"\n",
    "        )\n",
    "        answer = final.content\n",
    "    else:\n",
    "        answer = response.content\n",
    "\n",
    "    # Save assistant reply\n",
    "    chat_history.append(f\"Assistant: {answer}\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# ---------------- CHAT ----------------\n",
    "print(chat(\"Hi\"))\n",
    "print(chat(\"What is 5 plus 6?\"))\n",
    "print(chat(\"Add 10 more to that\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0371975-3d6c-4dc1-b9c4-c36e6ad2dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "4ï¸âƒ£ RAG Agent (Agent + Retriever)\n",
    "\n",
    "Not a separate class â€” it is:\n",
    "\n",
    "Agent + Vector DB Tool\n",
    "The agent decides:\n",
    "When to search\n",
    "When to answer\n",
    "\n",
    "Best for\n",
    "Document QA\n",
    "Policy bots\n",
    "Knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad743cea-ebec-4924-8bfc-f8febcd69526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is LangChain?\n",
      " LangChain is a framework designed for building applications that utilize large language models (LLMs). It provides tools and components to help developers create, manage, and deploy applications that leverage the capabilities of these models.\n",
      "What is RAG?\n",
      " RAG, or Retrieval Augmented Generation, is a technique that combines retrieval-based methods with generative models. It enhances the capabilities of language models by allowing them to access and incorporate external information from a knowledge base or database during the generation process. This approach helps improve the accuracy and relevance of the generated content by grounding it in real-world data.\n",
      "What is your name\n",
      " I am an AI language model and don't have a personal name. You can simply refer to me as \"Assistant.\" How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM (Brain)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- DOCUMENT STORE ----------------\n",
    "documents = {\n",
    "    \"langchain\": \"LangChain is a framework for building applications using LLMs.\",\n",
    "    \"langgraph\": \"LangGraph is used to build stateful multi-agent workflows.\",\n",
    "    \"rag\": \"RAG stands for Retrieval Augmented Generation.\"\n",
    "}\n",
    "\n",
    "# ---------------- RETRIEVER TOOL ----------------\n",
    "@tool\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search internal documents and return relevant text.\n",
    "    \"\"\"\n",
    "    for key, text in documents.items():\n",
    "        if key.lower() in query.lower():\n",
    "            return text\n",
    "    return \"No relevant document found.\"\n",
    "\n",
    "# ---------------- SIMPLE MEMORY ----------------\n",
    "chat_history = []\n",
    "\n",
    "def chat(user_input):\n",
    "    # Save user message\n",
    "    chat_history.append(f\"User: {user_input}\")\n",
    "\n",
    "    # Build conversation context\n",
    "    full_prompt = \"\\n\".join(chat_history)\n",
    "\n",
    "    # Ask LLM â€“ it decides whether to call search_docs\n",
    "    response = llm.bind_tools([search_docs]).invoke(full_prompt)\n",
    "\n",
    "    # If LLM wants to search documents\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        retrieved_text = search_docs.invoke(tool_call[\"args\"])\n",
    "\n",
    "        # Give retrieved knowledge back to LLM\n",
    "        final = llm.invoke(\n",
    "            f\"{full_prompt}\\nRetrieved info: {retrieved_text}\\nAnswer clearly.\"\n",
    "        )\n",
    "        answer = final.content\n",
    "    else:\n",
    "        answer = response.content\n",
    "\n",
    "    # Save assistant reply\n",
    "    chat_history.append(f\"Assistant: {answer}\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# ---------------- CHAT ----------------\n",
    "\n",
    "print(\"What is LangChain?\\n\",chat(\"What is LangChain?\"))\n",
    "print(\"What is RAG?\\n\",chat(\"What is RAG?\"))\n",
    "print(\"What is your name\\n\",chat(\"What is your name?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099ae83-0382-4185-8a74-71019630ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The model is forced to answer only from documents.\n",
    "If itâ€™s not in the docs â†’ it must say â€œI donâ€™t know.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f7931d-961c-4c0e-8f89-48ba156eb997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain: LangChain is a framework for building applications using LLMs.\n",
      "RAG: RAG stands for Retrieval Augmented Generation.\n",
      "Name: I donâ€™t know. This information is not in the documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- DOCUMENT STORE ----------------\n",
    "documents = {\n",
    "    \"langchain\": \"LangChain is a framework for building applications using LLMs.\",\n",
    "    \"langgraph\": \"LangGraph is used to build stateful multi-agent workflows.\",\n",
    "    \"rag\": \"RAG stands for Retrieval Augmented Generation.\"\n",
    "}\n",
    "\n",
    "# ---------------- SIMPLE RETRIEVER ----------------\n",
    "def retrieve(query):\n",
    "    for k, v in documents.items():\n",
    "        if k in query.lower():\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# ---------------- STRICT RAG ----------------\n",
    "def ask(question):\n",
    "    doc = retrieve(question)\n",
    "\n",
    "    # If no document found â†’ do NOT let LLM guess\n",
    "    if not doc:\n",
    "        return \"I donâ€™t know. This information is not in the documents.\"\n",
    "\n",
    "    # Force LLM to use only retrieved text\n",
    "    prompt = f\"\"\"\n",
    "Answer ONLY using the following document.\n",
    "\n",
    "Document:\n",
    "{doc}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the answer is not in the document, say \"I don't know.\"\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# ---------------- TEST ----------------\n",
    "print(\"LangChain:\", ask(\"What is LangChain?\"))\n",
    "print(\"RAG:\", ask(\"What is RAG?\"))\n",
    "print(\"Name:\", ask(\"What is your name?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570ddee-951a-4cdb-b853-dec6c22903e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "5ï¸âƒ£ Self-Ask Agent\n",
    "\n",
    "Breaks a big question into smaller ones and searches step-by-step.\n",
    "\n",
    "Mostly replaced by ReAct now.\n",
    "\n",
    "Main Question\n",
    "â†“\n",
    "Split into sub-questions\n",
    "â†“\n",
    "Search each\n",
    "â†“\n",
    "Combine answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2c9975-a553-4671-9e41-d2515bf9c2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-questions:\n",
      "1. Who founded LangChain?  \n",
      "2. What is LangChain?\n",
      "LangChain was founded by Harrison Chase and is a framework for building applications using large language models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- DOCUMENT STORE ----------------\n",
    "docs = {\n",
    "    \"founder\": \"LangChain was founded by Harrison Chase.\",\n",
    "    \"langchain\": \"LangChain is a framework for building applications using LLMs.\"\n",
    "}\n",
    "\n",
    "# ---------------- SEARCH TOOL ----------------\n",
    "@tool\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"Search internal knowledge\"\"\"\n",
    "    for key, value in docs.items():\n",
    "        if key.lower() in query.lower():\n",
    "            return value\n",
    "    return \"No information found.\"\n",
    "\n",
    "# ---------------- SELF-ASK AGENT ----------------\n",
    "def self_ask(question):\n",
    "    # Step 1: Ask LLM to split into sub-questions\n",
    "    breakdown = llm.invoke(\n",
    "        f\"Break this question into two simple questions: {question}\"\n",
    "    ).content\n",
    "\n",
    "    print(\"\\nSub-questions:\")\n",
    "    print(breakdown)\n",
    "\n",
    "    # Step 2: Ask LLM to pick search terms\n",
    "    sub_questions = [\n",
    "        \"founder\",\n",
    "        \"langchain\"\n",
    "    ]\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    # Step 3: Search for each sub-question\n",
    "    for q in sub_questions:\n",
    "        ans = search_docs.invoke({\"query\": q})\n",
    "        answers.append(ans)\n",
    "\n",
    "    # Step 4: Let LLM combine answers\n",
    "    final = llm.invoke(\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answers: {answers}\\n\"\n",
    "        \"Combine these into a single clear answer.\"\n",
    "    )\n",
    "\n",
    "    return final.content\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "print(self_ask(\"Who founded LangChain and what is it?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52096323-08c1-4258-8ab0-7fccc9f9edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6ï¸âƒ£ Plan-and-Execute Agent\n",
    "\n",
    "Two agents:\n",
    "\n",
    "Planner creates a plan\n",
    "Executor runs steps\n",
    "\n",
    "Best for\n",
    "\n",
    "Long workflows\n",
    "Business automation\n",
    "Multi-step tasks\n",
    "\n",
    "This shows:\n",
    "\n",
    "One LLM plans\n",
    "Another LLM executes step by step\n",
    "User question\n",
    "â†“\n",
    "Planner â†’ creates plan\n",
    "â†“\n",
    "Executor â†’ runs steps\n",
    "â†“\n",
    "Tools â†’ get data\n",
    "â†“\n",
    "Executor â†’ final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e448c28b-e6ab-417b-85f3-c087655ec1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plan:\n",
      " To effectively answer the question \"What is LangChain and what is LangGraph?\", you can follow this step-by-step plan:\n",
      "\n",
      "### Step 1: Define the Scope\n",
      "- **Objective**: Clearly outline what you want to achieve with your answer. You want to provide a comprehensive understanding of both LangChain and LangGraph.\n",
      "- **Target Audience**: Consider who will be reading your answer (e.g., developers, researchers, general public) to tailor the complexity of your explanation.\n",
      "\n",
      "### Step 2: Research LangChain\n",
      "- **Overview**: Start by gathering basic information about LangChain. Look for its purpose, features, and applications.\n",
      "- **Key Features**: Identify the main functionalities of LangChain, such as its capabilities in natural language processing, integration with other tools, and any unique selling points.\n",
      "- **Use Cases**: Find examples of how LangChain is used in real-world applications or projects.\n",
      "- **Official Documentation**: Check the official website or GitHub repository for the most accurate and up-to-date information.\n",
      "\n",
      "### Step 3: Research LangGraph\n",
      "- **Overview**: Similarly, gather information about LangGraph. Understand its purpose and how it relates to LangChain.\n",
      "- **Key Features**: Identify the main functionalities of LangGraph, including its capabilities, tools, and any unique aspects.\n",
      "- **Use Cases**: Look for examples of how LangGraph is applied in various scenarios.\n",
      "- **Official Documentation**: Again, refer to the official website or GitHub repository for accurate information.\n",
      "\n",
      "### Step 4: Compare and Contrast\n",
      "- **Relationship**: Determine how LangChain and LangGraph are related. Are they part of the same ecosystem? Do they serve complementary purposes?\n",
      "- **Differences**: Highlight the key differences between the two, such as their target audience, functionalities, and use cases.\n",
      "\n",
      "### Step 5: Organize Your Findings\n",
      "- **Structure**: Create a clear structure for your answer. A suggested format could be:\n",
      "  1. Introduction\n",
      "  2. What is LangChain?\n",
      "     - Overview\n",
      "     - Key Features\n",
      "     - Use Cases\n",
      "  3. What is LangGraph?\n",
      "     - Overview\n",
      "     - Key Features\n",
      "     - Use Cases\n",
      "  4. Comparison of LangChain and LangGraph\n",
      "  5. Conclusion\n",
      "\n",
      "### Step 6: Write the Answer\n",
      "- **Draft**: Write a draft based on your organized findings. Ensure clarity and conciseness.\n",
      "- **Technical Accuracy**: Make sure to use accurate terminology and provide correct information.\n",
      "\n",
      "### Step 7: Review and Edit\n",
      "- **Proofread**: Check for grammatical errors, clarity, and coherence.\n",
      "- **Feedback**: If possible, get feedback from someone knowledgeable in the field to ensure accuracy and completeness.\n",
      "\n",
      "### Step 8: Finalize and Share\n",
      "- **Final Edits**: Make any necessary adjustments based on feedback.\n",
      "- **Share**: Publish or present your answer in the appropriate format (e.g., blog post, presentation, report).\n",
      "\n",
      "### Step 9: Stay Updated\n",
      "- **Follow Up**: Keep an eye on updates related to LangChain and LangGraph, as technology evolves rapidly. This will help you maintain the relevance of your information.\n",
      "\n",
      "By following this plan, you can create a well-informed and structured response to the question about LangChain and LangGraph.\n",
      "\n",
      "Final Answer:\n",
      " LangChain is a framework designed for building applications that utilize large language models (LLMs), while LangGraph is a framework focused on creating stateful multi-agent workflows.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Planner LLM\n",
    "planner_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Executor LLM\n",
    "executor_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- DOCUMENT STORE ----------------\n",
    "docs = {\n",
    "    \"langchain\": \"LangChain is a framework for building applications using LLMs.\",\n",
    "    \"langgraph\": \"LangGraph is a framework for building stateful multi-agent workflows.\"\n",
    "}\n",
    "\n",
    "# ---------------- SEARCH TOOL ----------------\n",
    "@tool\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"Search internal knowledge\"\"\"\n",
    "    for k, v in docs.items():\n",
    "        if k.lower() in query.lower():\n",
    "            return v\n",
    "    return \"Not found.\"\n",
    "\n",
    "# ---------------- PLANNER ----------------\n",
    "def planner(question):\n",
    "    plan = planner_llm.invoke(\n",
    "        f\"Create a step-by-step plan to answer: {question}\"\n",
    "    ).content\n",
    "    return plan\n",
    "\n",
    "# ---------------- EXECUTOR ----------------\n",
    "def executor(question, plan):\n",
    "    print(\"\\nPlan:\\n\", plan)\n",
    "\n",
    "    steps = [\"langchain\", \"langgraph\"]\n",
    "    results = []\n",
    "\n",
    "    for step in steps:\n",
    "        result = search_docs.invoke({\"query\": step})\n",
    "        results.append(result)\n",
    "\n",
    "    final = executor_llm.invoke(\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Results: {results}\\n\"\n",
    "        \"Combine these into a clear answer.\"\n",
    "    )\n",
    "    return final.content\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "question = \"What is LangChain and what is LangGraph?\"\n",
    "\n",
    "plan = planner(question)\n",
    "answer = executor(question, plan)\n",
    "\n",
    "print(\"\\nFinal Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e0661-a3be-49dc-aba4-29b8c4c4a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "7ï¸âƒ£ Multi-Tool Agent\n",
    "\n",
    "One agent using many tools:\n",
    "\n",
    "DB\n",
    "APIs\n",
    "Python\n",
    "Search\n",
    "Vector DB\n",
    "\n",
    "This is how real enterprise agents are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ebb998-a33c-4165-97b4-c9ba73a994f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for developing applications that utilize large language models (LLMs).\n",
      "5 plus 7 equals 12.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM (Brain)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ---------------- KNOWLEDGE BASE ----------------\n",
    "docs = {\n",
    "    \"langchain\": \"LangChain is a framework for building applications using LLMs.\"\n",
    "}\n",
    "\n",
    "# ---------------- TOOL 1: SEARCH ----------------\n",
    "@tool\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"Search internal documents\"\"\"\n",
    "    for k, v in docs.items():\n",
    "        if k in query.lower():\n",
    "            return v\n",
    "    return \"Not found.\"\n",
    "\n",
    "# ---------------- TOOL 2: MATH ----------------\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools = [search_docs, add_numbers]\n",
    "\n",
    "# ---------------- MULTI-TOOL AGENT ----------------\n",
    "def ask(question):\n",
    "    # LLM decides which tool to call\n",
    "    response = llm.bind_tools(tools).invoke(question)\n",
    "\n",
    "    # If tool is needed\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        args = tool_call[\"args\"]\n",
    "\n",
    "        # Run correct tool\n",
    "        tool_map = {\n",
    "            \"search_docs\": search_docs,\n",
    "            \"add_numbers\": add_numbers\n",
    "        }\n",
    "        tool_result = tool_map[tool_name].invoke(args)\n",
    "\n",
    "        # Send tool result back to LLM\n",
    "        final = llm.invoke(\n",
    "            f\"Question: {question}\\nTool result: {tool_result}\\nAnswer clearly.\"\n",
    "        )\n",
    "        return final.content\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "print(ask(\"What is LangChain?\"))\n",
    "print(ask(\"What is 5 plus 7?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af4fc3-69ef-4c9e-83b0-be42a9203aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Human-in-the-Loop Agent\n",
    "Scenario:\n",
    "\n",
    "AI calculates a value\n",
    "Human must approve before final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0879671-74ce-4304-93c1-c5fc78440106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI suggests: 42\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Approve? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human rejected the AI result.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load API key\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ----------- TOOL -----------\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# ----------- HUMAN APPROVAL TOOL -----------\n",
    "def human_approval(result):\n",
    "    print(\"\\nAI suggests:\", result)\n",
    "    decision = input(\"Approve? (yes/no): \")\n",
    "    return decision.lower() == \"yes\"\n",
    "\n",
    "# ----------- HUMAN-IN-THE-LOOP AGENT -----------\n",
    "def ask(question):\n",
    "    # Step 1: LLM decides to use tool\n",
    "    response = llm.bind_tools([add_numbers]).invoke(question)\n",
    "\n",
    "    # Step 2: Run tool\n",
    "    tool_call = response.tool_calls[0]\n",
    "    tool_result = add_numbers.invoke(tool_call[\"args\"])\n",
    "\n",
    "    # Step 3: Human review\n",
    "    if not human_approval(tool_result):\n",
    "        return \"Human rejected the AI result.\"\n",
    "\n",
    "    # Step 4: Final answer after approval\n",
    "    final = llm.invoke(\n",
    "        f\"The approved result is {tool_result}. Answer the user.\"\n",
    "    )\n",
    "    return final.content\n",
    "\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "print(ask(\"What is 40 plus 2?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d25d73b-73c9-41ac-a107-071c8c0bd1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI suggests: 42\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Approve? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approved result is 42. If you have any further questions or need additional information, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"What is 40 plus 2?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cb628-3d64-45d6-81f8-ebcfe5ff8c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainai)",
   "language": "python",
   "name": "langchainai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
