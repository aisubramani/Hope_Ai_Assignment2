{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0307bf-39d9-4401-86bf-d7e3c8a39186",
   "metadata": {},
   "source": [
    "# LangChain V1 - CHAIN, TOOL, and AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4ae0a-1b95-4ac3-a1e4-2f7c8be116d9",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ CHAIN ‚Äî Simple Question ‚Üí Answer\n",
    "\n",
    "CHAIN EXAMPLE (LangChain v1 - Modern LCEL)\n",
    "\n",
    "What this does:\n",
    "- Takes a question\n",
    "- Sends it to the LLM\n",
    "- Gets a direct answer\n",
    "\n",
    "No tools.\n",
    "No decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a21cefd-eee8-4759-8c78-77ad7be838dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Framework for LLMs**: LangChain is a framework designed to facilitate the development of applications using large language models (LLMs) by providing tools and components for integration and interaction.\n",
      "- **Modular Components**: It offers modular components such as document loaders, text splitters, and chains to streamline the process of building LLM-powered applications.\n",
      "- **Use Cases**: LangChain supports various use cases, including chatbots, question-answering systems, and data analysis, enabling developers to create sophisticated AI-driven solutions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Initialize LLM\n",
    "# temperature=0 makes output stable and predictable\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create a prompt template\n",
    "# {topic} will be replaced at runtime\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Answer in a short list (max 3 bullets): What is {topic}?\"\n",
    ")\n",
    "\n",
    "# Create a CHAIN using LCEL (| operator)\n",
    "# This means: prompt ‚Üí LLM\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"topic\": \"LangChain\"})\n",
    "\n",
    "# Print only the text answer\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d868519-7c74-40f7-af4d-eee9864cc1cf",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ TOOL ‚Äî Simple Python Function\n",
    "\n",
    "TOOL EXAMPLE (LangChain v1 - Modern)\n",
    "\n",
    "What this does:\n",
    "- Defines a simple Python function\n",
    "- Wraps it as a LangChain tool\n",
    "- No LLM involved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95d254c-77d9-43c2-bfdb-f39795c72c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: 12\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# @tool makes this function usable by agents\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers and return the result.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Call the tool directly\n",
    "# Tools can be used without an agent\n",
    "result = add_numbers.invoke({\"a\": 5, \"b\": 7})\n",
    "\n",
    "print(\"Tool result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa3db8-577c-47dc-885b-2ca6d0b0202b",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ AGENT ‚Äî LLM Decides to Use Tool\n",
    "\n",
    "AGENT EXAMPLE (LangChain v1 - Modern)\n",
    "\n",
    "What this does:\n",
    "- LLM reads the user question\n",
    "- Decides if a tool is needed\n",
    "- Calls the tool automatically\n",
    "- Returns the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efad7321-a9f7-4cdc-8a17-8e09f90afec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use tools when needed.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Bind tools to LLM (this enables tool calling)\n",
    "llm_with_tools = llm.bind_tools([add_numbers])\n",
    "\n",
    "# Build runnable chain\n",
    "chain = prompt | llm_with_tools\n",
    "\n",
    "# Run\n",
    "result = chain.invoke({\n",
    "    \"input\": \"What is 10 plus 15?\"\n",
    "})\n",
    "\n",
    "# If tool was called, execute it\n",
    "if result.tool_calls:\n",
    "    call = result.tool_calls[0]\n",
    "    output = add_numbers.invoke(call[\"args\"])\n",
    "    print(output)\n",
    "else:\n",
    "    print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9034a5-07a1-45a0-8b64-0129f400bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LangChain v1: Multi-Tool System (QA, Summarization & Web Search) Using Runnable Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "098ffecd-e744-4855-ab3b-42f9892fb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15016\\AppData\\Local\\Temp\\ipykernel_36244\\1197590838.py:55: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Simple QA Tool Output:\n",
      " LangGraph in LangChain is a framework that allows users to create and manage complex workflows by connecting different language model components and data sources, enabling more sophisticated interactions and data processing.\n",
      "\n",
      "üìù Summarizer Tool Output:\n",
      " LangGraph is a framework that enables the creation of stateful multi-step agents with LangChain, utilizing a graph-based design to represent agent workflows and memory.\n",
      "\n",
      "üåê Web Search Tool Output:\n",
      " [{'title': 'Release policy - Docs by LangChain', 'url': 'https://docs.langchain.com/oss/python/release-policy', 'content': 'light logo\\ndark logo\\n\\nPython\\n\\n##### Reference\\n\\n##### Errors\\n\\n##### Releases\\n\\n##### Policies\\n\\n# Release policy\\n\\n`langchain-core`\\n`langchain`\\n`langchain-community`\\n\\n## \\u200b Release cadence\\n\\n`1.0.x`\\n`1.1.0`\\n`langchain`\\n`langchain-core`\\n\\n## \\u200b API stability\\n\\n`langchain`\\n`langchain-core`\\n`2.0.0`\\n`1.0.0`\\n`1.1.0`\\n`1.0.0`\\n`1.0.1`\\n\\n### \\u200b Stability of other packages\\n\\n`langchain-openai`\\n`langchain-anthropic`\\n`langchain-community`\\n`langchain-community`\\n`langchain`\\n`langchain-core`\\n\\n## \\u200b Deprecation policy\\n\\n`langchain`\\n`langchain-core`\\n\\n## \\u200b Long-term support (LTS)\\n\\n### \\u200b Release status definitions\\n\\n### \\u200b Current LTS releases\\n\\n### \\u200b Legacy version support\\n\\n### \\u200b Special considerations\\n\\n`langchain-community`\\n\\n## \\u200b Release cadence\\n\\n## \\u200b API stability\\n\\n### \\u200b Stable APIs\\n\\n### \\u200b Beta features\\n\\n`beta` [...] ### \\u200b Stable APIs\\n\\n### \\u200b Beta features\\n\\n`beta`\\n\\n### \\u200b Experimental features\\n\\n`experimental`\\n`alpha`\\n\\n### \\u200b Internal APIs\\n\\n`_`\\n\\n## \\u200b Deprecation policy\\n\\n## \\u200b Platform compatibility\\n\\n### \\u200b Python support\\n\\n## \\u200b Breaking changes\\n\\n## \\u200b Migration support\\n\\n## \\u200b Long-term support (LTS)\\n\\n### \\u200b Release status definitions\\n\\n### \\u200b Current LTS releases\\n\\n### \\u200b Legacy version support\\n\\n## \\u200b See also\\n\\nWas this page helpful?\\n\\nlight logo\\ndark logo\\n\\nResources\\n\\nCompany', 'score': 0.51091194}, {'title': 'State of Agent Engineering - LangChain', 'url': 'https://www.langchain.com/state-of-agent-engineering', 'content': '## Insights\\n\\n### Large enterprises are leading adoption\\n\\nMore than half of respondents surveyed (57.3%) now have agents running in production environments, with another 30.4% actively developing agents with concrete plans to deploy them.\\n\\nThis marks clear growth from last year‚Äôs survey, where 51% reported having agents in production. Organizations are moving past the proof-of-concept stage into production ‚Äî  the question for most organizations is no longer ‚Äúif‚Äù they will ship agents but ‚Äúhow‚Äù and ‚Äúwhen‚Äù.\\n\\n\\u200d\\n\\nWhat changes at scale? [...] Latency has emerged as second biggest challenge (20%). As agents move into customer-facing use cases like customer service and code generation, response time becomes a critical part of the user experience. This also reflects the tradeoff between quality and speed for teams, as more capable, multi-step agents can deliver higher quality outputs but often with slower responses.\\n\\nIn contrast, cost is less frequently cited as a concern than in previous years. Falling model prices and improved efficiency appear to have shifted attention away from raw spend, with organizations prioritizing making agents work well and fast.\\n\\n\\u200d\\n\\nWhat changes at scale? [...] The strong showing of customer service suggests a shift toward teams putting agents directly in front of customers, not just using them internally. At the same time, agents continue to deliver clear value internally, with 18% citing using agents for internal workflow automation to boost employee efficiency.\\n\\nThe popularity of research & data analysis use cases further reinforces where agents shine today: synthesizing large volumes of information, reasoning across sources, and accelerating knowledge-intensive tasks.\\n\\nNotably, we saw a greater spread of use cases selected by respondents this year (respondents could only select one primary use case), so agent adoption may be diversifying beyond a narrow set of early applications.\\n\\n\\u200d\\n\\nWhat changes at scale?', 'score': 0.47504577}, {'title': 'LangChain', 'url': 'https://www.langchain.com/', 'content': \"#### Code generation\\n\\nAccelerate software development by automating code writing, refactoring, and documentation for your team.\\n\\n#### AI Search\\n\\nOffer a concierge experience to guide users to products or information in a personalized way.\\n\\n## Get inspired by companies who have done it\\n\\nTeams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.\\n\\nKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\\n\\nElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\\n\\nRakuten‚Äôs GenAI platform, built with LangGraph and LangSmith, lets employees across 70+ businesses create AI agents. [...] Zip.svg)\\nWriter\\nHarvey\\nVanta\\nAbridge.svg)\\nClay\\nRippling\\nMercor\\ndbt Labs\\nKlarna\\nHeadspace.svg)\\nLyft.svg)\\nCoinbase.svg)\\nRakuten\\nLinkedIn\\nElastic\\nWorkday\\nMonday.com\\nNU\\nBridgewater\\nCloudflare\\nGitLab%201.svg)\\nThe Home Depot\\nCisco\\nBristol Myers Squibb.svg)\\nZip.svg)\\nWriter\\nHarvey\\nVanta\\nAbridge.svg)\\nClay\\nRippling\\nMercor\\ndbt Labs\\nKlarna\\nHeadspace.svg)\\nLyft.svg)\\nZip.svg)\\nWriter\\nHarvey\\nVanta\\nAbridge.svg)\\nClay\\nRippling\\nMercor\\ndbt Labs\\nKlarna\\nHeadspace.svg)\\nLyft.svg)\\nCoinbase.svg)\\nRakuten\\nLinkedIn\\nElastic\\nWorkday\\nMonday.com\\nNU\\nBridgewater\\nCloudflare\\nGitLab%201.svg)\\nThe Home Depot\\nCisco\\nBristol Myers Squibb.svg)\\nCoinbase.svg)\\nRakuten\\nLinkedIn\\nElastic\\nWorkday\\nMonday.com\\nNU\\nBridgewater\\nCloudflare\\nGitLab%201.svg)\\nThe Home Depot\\nCisco\\nBristol Myers Squibb.svg)\\n\\n#### Visibility & control [...] #### Learn alongside the 1 million+ practitioners who are pushing the industry forward\\n\\n#### 90M\\n\\n#### Monthly downloads\\n\\n#### 100k+\\n\\n#### GitHub stars\\n\\n#### #1\\n\\n#### Downloaded agent framework\\n\\n#### 1000\\n\\n#### Integrations\\n\\n## Ready to start shipping \\u2028reliable agents faster?\\n\\nGet started with tools from the LangChain product suite for every step of the agent development lifecycle.\", 'score': 0.43467787}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# üîê Load API keys\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# üî∏ Initialize LLM (modern model)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# ‚úÖ Tool 1: Simple QA Tool\n",
    "# =====================================================\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"Answer clearly with short form: {question}\"\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "@tool\n",
    "def simple_qa(question: str) -> str:\n",
    "    \"\"\"Answer factual questions clearly\"\"\"\n",
    "    response = qa_chain.invoke({\"question\": question})\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ‚úÖ Tool 2: Summarizer Tool\n",
    "# =====================================================\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"short Summarize the following text:\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm\n",
    "\n",
    "@tool\n",
    "def summarizer(text: str) -> str:\n",
    "    \"\"\"Summarizes input text\"\"\"\n",
    "    response = summary_chain.invoke({\"text\": text})\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ‚úÖ Tool 3: Web Search Tool (Tavily)\n",
    "# =====================================================\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the internet for current information\"\"\"\n",
    "    results = tavily_search.invoke({\"query\": query})\n",
    "    return str(results)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# üß™ Tool usage examples\n",
    "# =====================================================\n",
    "\n",
    "qa_query = \"What is LangGraph in LangChain?\"\n",
    "\n",
    "summary_text = \"\"\"\n",
    "LangGraph is a framework for building stateful multi-step agents using LangChain.\n",
    "It uses graph-based design to model agent workflows and memory.\n",
    "\"\"\"\n",
    "\n",
    "search_query = \"Latest updates on LangChain\"\n",
    "\n",
    "print(\"\\nüß† Simple QA Tool Output:\\n\", simple_qa.invoke(qa_query))\n",
    "print(\"\\nüìù Summarizer Tool Output:\\n\", summarizer.invoke(summary_text))\n",
    "print(\"\\nüåê Web Search Tool Output:\\n\", web_search.invoke(search_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da27ff-b88f-4b50-8de5-ad9f7f55eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchainai)",
   "language": "python",
   "name": "langchainai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
