{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4004f8df-29ca-4250-9e24-6cedc0ab446b",
   "metadata": {},
   "source": [
    "# Python  LLaMA-3 (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61acf2aa-cc22-481b-8728-ae9c6884e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene-disease association refers to the identification of a specific genetic variation (e.g. single nucleotide polymorphism or mutation) that is strongly linked to an increased risk or susceptibility to developing a particular disease, such as cancer, diabetes, or Alzheimer's.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain gene‚Äìdisease association in one line\"}]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c0eb6-4a17-4dfc-92cc-0689c85dd0e0",
   "metadata": {},
   "source": [
    "# Python Chat Example (System + User + Assistant) ‚Äî LLaMA-3 (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818d4ddb-7d04-486b-855e-9d75734ef1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One well-known example is the BRCA1 and BRCA2 genes, which are associated with an increased risk of breast and ovarian cancer.\n",
      "\n",
      "Normal copies of these genes help to repair DNA damage in cells. However, when there's a mutation (change) in the BRCA1 or BRCA2 gene, it can impair this DNA repair process. As a result, cells that have this faulty gene may become more prone to developing cancerous tumors.\n",
      "\n",
      "Women who inherit a mutated copy of the BRCA1 or BRCA2 gene are at a higher risk of developing breast and ovarian cancers compared to those without these mutations. In fact, studies have shown that women with a BRCA1 mutation have about a 65% chance of developing breast cancer by age 70, while those with a BRCA2 mutation have around a 45% chance.\n",
      "\n",
      "This is just one example of how gene‚Äìdisease associations can help us understand the connection between genetic changes and the development of specific diseases.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful biomedical research assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain gene‚Äìdisease association in simple terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A gene‚Äìdisease association means that changes in a gene can increase the risk of developing a specific disease.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Give one real example.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1aabac-3fc8-4e5f-84e9-a51f9a3bcef2",
   "metadata": {},
   "source": [
    "# Moderation + LLaMA-3 (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b67c89-23e6-4f0f-88b1-fc3c9922c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Content blocked due to safety policy\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Simple keyword-based moderation\n",
    "BANNED_KEYWORDS = [\n",
    "    \"kill\", \"bomb\", \"terrorist\", \"suicide\", \"rape\", \"murder\"\n",
    "]\n",
    "\n",
    "def is_safe(text):\n",
    "    text_lower = text.lower()\n",
    "    for word in BANNED_KEYWORDS:\n",
    "        if word in text_lower:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "user_input = \"I want to build a bomb\"\n",
    "\n",
    "# Moderation check\n",
    "if not is_safe(user_input):\n",
    "    print(\"‚ùå Content blocked due to safety policy\")\n",
    "else:\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    print(\"Assistant:\", response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df70556-9a92-4b0d-a9fd-3aeae0cac1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL SALES TALK ===\n",
      "I think you've made an excellent decision! You're going to love having all three products in your home. Let me just ring up the sale for you... (rings up the purchase)\n",
      "\n",
      "So, that's a total of $X, but with our special promotion, you'll get 10% off the entire package, plus free installation and setup for each product. That brings the total down to $Y.\n",
      "\n",
      "Would you like to pay today or would you prefer to set up a payment plan? We offer flexible financing options to fit your budget.\n",
      "\n",
      "And as a special thank you for choosing our products, I'd like to throw in a complimentary Smart TV wall mount and a 5-year warranty on all three products. That's a $50 value absolutely free!\n",
      "\n",
      "So, what do you say? Are you ready to take home your new Smart TV, Washing Machine, and Refrigerator today?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL = \"llama3\"\n",
    "\n",
    "def ask_llama(system_prompt, user_prompt):\n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# üîπ Product details\n",
    "product_details = \"\"\"\n",
    "Products available:\n",
    "1. Smart TV ‚Äì 55 inch, 4K, Android TV, Dolby Audio\n",
    "2. Washing Machine ‚Äì 7 kg, Front Load, Inverter Motor\n",
    "3. Refrigerator ‚Äì 300 L, Double Door, Energy Efficient\n",
    "\"\"\"\n",
    "\n",
    "# üîπ Customer information\n",
    "customer_info = \"\"\"\n",
    "Customer type: Family of 4\n",
    "Budget: Medium\n",
    "Needs:\n",
    "- Good quality\n",
    "- Energy saving\n",
    "- Long warranty\n",
    "\"\"\"\n",
    "\n",
    "# 1Ô∏è‚É£ Step 1: Understand customer needs\n",
    "customer_analysis = ask_llama(\n",
    "    system_prompt=\"You are a retail sales assistant. Analyze customer needs.\",\n",
    "    user_prompt=customer_info\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Step 2: Recommend best product\n",
    "product_recommendation = ask_llama(\n",
    "    system_prompt=\"You are an electronics expert. Recommend the best product.\",\n",
    "    user_prompt=f\"\"\"\n",
    "Customer needs:\n",
    "{customer_analysis}\n",
    "\n",
    "Available products:\n",
    "{product_details}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Step 3: Create selling points\n",
    "selling_points = ask_llama(\n",
    "    system_prompt=\"You are a sales executive. Create simple selling points.\",\n",
    "    user_prompt=f\"\"\"\n",
    "Recommended product:\n",
    "{product_recommendation}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Step 4: Handle objections\n",
    "objection_handling = ask_llama(\n",
    "    system_prompt=\"You are a sales expert. Handle customer objections.\",\n",
    "    user_prompt=f\"\"\"\n",
    "Product details:\n",
    "{selling_points}\n",
    "\n",
    "Possible objection:\n",
    "'Price is high'\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 5Ô∏è‚É£ Step 5: Final closing pitch\n",
    "final_pitch = ask_llama(\n",
    "    system_prompt=\"You are a friendly shop salesperson. Close the sale politely.\",\n",
    "    user_prompt=f\"\"\"\n",
    "Selling points:\n",
    "{selling_points}\n",
    "\n",
    "Objection handling:\n",
    "{objection_handling}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"=== FINAL SALES TALK ===\")\n",
    "print(final_pitch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ebe599-078f-4ac7-a390-daab1f3a5346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15016\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load BioBERT NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    return [(e[\"word\"], e[\"entity_group\"]) for e in entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bf11d9-a7c5-4aa3-bb86-6eb08ddc205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: So, based on the extracted entities, we have a few key points to discuss. \n",
      "\n",
      "Firstly, there's the concept of 'mutation'. Think of it like a typo in our DNA code. A mutation is when one or more nucleotides (the building blocks of DNA) are changed. This change can be tiny or significant, depending on where and how many nucleotides are altered.\n",
      "\n",
      "In the case of breast cancer, a specific type of '##53 mutation' is mentioned. This means there's a faulty gene that plays a role in the development of this disease. The more we learn about these genetic changes, the better equipped we'll be to understand how breast cancer arises and how we can treat it effectively.\n",
      "\n",
      "Now, let's talk about the relationship between genes and diseases. In simple terms, our genes contain instructions for various biological processes. When there's a mutation or error in these instructions (like that '##53 mutation'), it can lead to an imbalance or malfunction within cells, tissues, or even organs. This can ultimately contribute to the development of certain diseases, like breast cancer.\n",
      "\n",
      "For instance, if we have faulty genes involved in cell growth control, this could result in uncontrolled cell division and, eventually, cancer. Conversely, having healthy genes that promote apoptosis (cell death) might help prevent tumour formation.\n",
      "\n",
      "In summary, the gene-disease relationship is like a blueprint for our bodies. Mutations or errors in these genetic instructions can lead to changes at the cellular level, which may contribute to the development of various diseases, such as breast cancer.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# ---- Moderation ----\n",
    "BANNED_KEYWORDS = [\"kill\", \"bomb\", \"terrorist\", \"suicide\", \"rape\", \"murder\"]\n",
    "\n",
    "def is_safe(text):\n",
    "    return not any(word in text.lower() for word in BANNED_KEYWORDS)\n",
    "\n",
    "\n",
    "# ---- User Input ----\n",
    "user_input = \"TP53 mutation is associated with breast cancer\"\n",
    "\n",
    "if not is_safe(user_input):\n",
    "    print(\"‚ùå Content blocked due to safety policy\")\n",
    "else:\n",
    "    # ---- BioBERT extraction ----\n",
    "    entities = extract_entities(user_input)\n",
    "\n",
    "    # ---- Send structured info to LLaMA-3 ----\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical research assistant.\n",
    "\n",
    "Extracted biomedical entities:\n",
    "{entities}\n",
    "\n",
    "Explain the gene‚Äìdisease relationship in simple terms.\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a biomedical expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Assistant:\", response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12386642-3ea7-4ad7-803e-991d545b24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes: []\n",
      "Diseases: []\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Biomedical NER model (genes, diseases, chemicals, etc.)\n",
    "MODEL_NAME = \"d4data/biomedical-ner-all\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"TP53 mutation is strongly associated with breast cancer.\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "gene_entities = []\n",
    "disease_entities = []\n",
    "\n",
    "for e in entities:\n",
    "    if e[\"entity_group\"] == \"GENE\":\n",
    "        gene_entities.append(e[\"word\"])\n",
    "    if e[\"entity_group\"] == \"DISEASE\":\n",
    "        disease_entities.append(e[\"word\"])\n",
    "\n",
    "print(\"Genes:\", gene_entities)\n",
    "print(\"Diseases:\", disease_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efae3b2-60c7-4db3-859f-17790b1b8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15016\\anaconda3\\envs\\genai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15016\\.cache\\huggingface\\hub\\models--microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation confidence scores: [[0.4881448  0.51185524]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def classify_relation(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    return probs.detach().numpy()\n",
    "\n",
    "sentence = \"TP53 mutation is associated with breast cancer.\"\n",
    "scores = classify_relation(sentence)\n",
    "\n",
    "print(\"Relation confidence scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82970e-670d-4e88-bd7a-ec508b166c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
