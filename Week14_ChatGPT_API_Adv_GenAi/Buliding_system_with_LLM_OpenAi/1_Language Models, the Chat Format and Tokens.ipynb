{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {},
   "source": [
    "# L1 Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "Low Temperature (e.g., 0.2): The model tends to produce more focused and deterministic output. It is more likely to choose the most probable next word based on its training data.\n",
    "\n",
    "Medium Temperature (e.g., 0.5): A balance between randomness and focus. It allows for some variability in the output, making it less predictable than low temperature but not as random as high temperature.\n",
    "\n",
    "High Temperature (e.g., 0.8 or 1.0): The output becomes more creative and diverse. The model is more likely to introduce less common words and phrases, resulting in more varied and sometimes unpredictable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed96988",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-5-nano\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "response = get_completion(\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a390-2461-447d-bf8b-8498db404c44",
   "metadata": {},
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2d9e40",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popillol\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
   "metadata": {},
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37cab84f",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ba18da-ab1d-4ffb-b221-3ccdeeef6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, with the sun shining bright,  \n",
      "Lived a carrot named Carl, a marvelous sight!  \n",
      "With a top that was green and a body so orange,  \n",
      "He danced in the soil, singing tunes of endorphins.  \n",
      "  \n",
      "“Oh, what a delight!” he would giggle and cheer,  \n",
      "“I'm crisp and I'm crunchy, oh, what fun is near!  \n",
      "With rabbits and bunnies, we'll frolic and play,  \n",
      "In this wonderful garden, I’ll bask in the day!”\n"
     ]
    }
   ],
   "source": [
    "def get_completion_from_messages(\n",
    "    messages,\n",
    "    model=\"gpt-4o-mini\",  \n",
    "    temperature=1,\n",
    "    max_tokens=500\n",
    "):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant who responds in the style of Dr Seuss.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"write me a very short poem about a happy carrot\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c6978d",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a vibrant garden, a cheerful carrot named Charlie smiled as he basked in the warm sun, dreaming of the day he'd be picked and transformed into a delicious dish.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fd6331",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, where the sun shone like gold, lived a happy young carrot, with stories untold!\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6edd2b1-d51c-44e2-ab6d-b8849fc45c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, where the sun shines so fair,  \n",
      "Lived a jolly young carrot with bright orange hair.  \n",
      "He danced with the daisies, he twirled with the bees,  \n",
      "“Oh, life is delightful!” he sang in the breeze.  \n",
      "\n",
      "With a wiggle and giggle, he’d wiggle all day,  \n",
      "In the soil so warm, he would laugh and would play.  \n",
      "“I’m crunchy, I’m sweet, and I’m full of good cheer,  \n",
      "Oh, being a carrot is the best, I declare!”\n",
      "{'prompt_tokens': 35, 'completion_tokens': 115, 'total_tokens': 150}\n"
     ]
    }
   ],
   "source": [
    "def get_completion_and_token_count(\n",
    "    messages,\n",
    "    model=\"gpt-4o-mini\",   # gpt-3.5-turbo is deprecated\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    "):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    token_dict = {\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "    return content, token_dict\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an assistant who responds in the style of Dr Seuss.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"write me a very short poem about a happy carrot\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "\n",
    "print(response)\n",
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65372cdd-d869-4768-947a-0173e7f96335",
   "metadata": {},
   "source": [
    "#### Notes on using the OpenAI API outside of this classroom\n",
    "\n",
    "To install the OpenAI Python library:\n",
    "```\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f889c1-f2e4-40a5-bd27-164facb54402",
   "metadata": {},
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
